{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are hired by an international bank with millions of customers spread all around Europe mainly in, three countries, Spain, France and Germany. In the last six months the bank detected that the number of people leaving the bank started to increase, so they decided to take measures. \n",
    "\n",
    "The bank decided to take a small sample of 10,000 of their customers and retrieve some information.\n",
    "For six months they followed the behaviour of these 10,000 customers and analysed which stayed and who left the bank. \n",
    "\n",
    "Therefore, they want you to develop a model that can measure the probability of a customer leaving the bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start by importing numpy and pandas -- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import the dataset Bank_customers.csv**\n",
    "\n",
    "**Perform all required data preprocessing steps, until you have your train set and yout test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9995</td>\n",
       "      <td>9996</td>\n",
       "      <td>15606229</td>\n",
       "      <td>Obijiaku</td>\n",
       "      <td>771</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9996</td>\n",
       "      <td>9997</td>\n",
       "      <td>15569892</td>\n",
       "      <td>Johnstone</td>\n",
       "      <td>516</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9997</td>\n",
       "      <td>9998</td>\n",
       "      <td>15584532</td>\n",
       "      <td>Liu</td>\n",
       "      <td>709</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9998</td>\n",
       "      <td>9999</td>\n",
       "      <td>15682355</td>\n",
       "      <td>Sabbatini</td>\n",
       "      <td>772</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9999</td>\n",
       "      <td>10000</td>\n",
       "      <td>15628319</td>\n",
       "      <td>Walker</td>\n",
       "      <td>792</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RowNumber  CustomerId    Surname  CreditScore  Age  Tenure    Balance  \\\n",
       "0             1    15634602   Hargrave          619   42       2       0.00   \n",
       "1             2    15647311       Hill          608   41       1   83807.86   \n",
       "2             3    15619304       Onio          502   42       8  159660.80   \n",
       "3             4    15701354       Boni          699   39       1       0.00   \n",
       "4             5    15737888   Mitchell          850   43       2  125510.82   \n",
       "...         ...         ...        ...          ...  ...     ...        ...   \n",
       "9995       9996    15606229   Obijiaku          771   39       5       0.00   \n",
       "9996       9997    15569892  Johnstone          516   35      10   57369.61   \n",
       "9997       9998    15584532        Liu          709   36       7       0.00   \n",
       "9998       9999    15682355  Sabbatini          772   42       3   75075.31   \n",
       "9999      10000    15628319     Walker          792   28       4  130142.79   \n",
       "\n",
       "      NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  Exited  \\\n",
       "0                 1          1               1        101348.88       1   \n",
       "1                 1          0               1        112542.58       0   \n",
       "2                 3          1               0        113931.57       1   \n",
       "3                 2          0               0         93826.63       0   \n",
       "4                 1          1               1         79084.10       0   \n",
       "...             ...        ...             ...              ...     ...   \n",
       "9995              2          1               0         96270.64       0   \n",
       "9996              1          1               1        101699.77       0   \n",
       "9997              1          0               1         42085.58       1   \n",
       "9998              2          1               0         92888.52       1   \n",
       "9999              1          1               0         38190.78       0   \n",
       "\n",
       "      Geography_Germany  Geography_Spain  Gender_Male  \n",
       "0                     0                0            0  \n",
       "1                     0                1            0  \n",
       "2                     0                0            0  \n",
       "3                     0                0            0  \n",
       "4                     0                1            0  \n",
       "...                 ...              ...          ...  \n",
       "9995                  0                0            1  \n",
       "9996                  0                0            1  \n",
       "9997                  0                0            0  \n",
       "9998                  1                0            1  \n",
       "9999                  0                0            0  \n",
       "\n",
       "[10000 rows x 15 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Bank_customers.csv')\n",
    "categorical_features = ['Geography','Gender']\n",
    "final_df = pd.get_dummies(df,columns=categorical_features,drop_first=True)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()\n",
    "X = final_df.drop(['RowNumber','CustomerId','Surname','Exited'],axis=1) \n",
    "y = final_df['Exited']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform feature scaling\n",
    "\n",
    "Use the StandardScaler class from sklearn.preprocessing. You can read more about it here https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "\n",
    "Use the fit_transform and the transform methods to perform feature scalng on your training set and your test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.60237034e-02,  9.86670878e-04, -1.72917010e+00, ...,\n",
       "         1.70776143e+00, -5.72218165e-01,  9.06973121e-01],\n",
       "       [-1.32109201e+00, -4.76434722e-01, -1.04444065e+00, ...,\n",
       "        -5.85561885e-01, -5.72218165e-01,  9.06973121e-01],\n",
       "       [ 6.84533946e-02, -5.71919000e-01, -3.59711204e-01, ...,\n",
       "        -5.85561885e-01,  1.74758521e+00,  9.06973121e-01],\n",
       "       ...,\n",
       "       [-4.64379805e-01, -3.80950443e-01,  3.25018245e-01, ...,\n",
       "        -5.85561885e-01, -5.72218165e-01,  9.06973121e-01],\n",
       "       [ 6.22182014e-01, -1.52676179e+00,  1.00974769e+00, ...,\n",
       "         1.70776143e+00, -5.72218165e-01,  9.06973121e-01],\n",
       "       [ 2.09530909e+00,  2.57906219e+00, -1.72917010e+00, ...,\n",
       "         1.70776143e+00, -5.72218165e-01, -1.10256851e+00]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "sc.fit(X_train,y_train)\n",
    "sc.transform(X)\n",
    "sc.fit_transform(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Now let's create the ANN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Keras libraries and packages\n",
    "You are going to use the Sequential model. You can read more about it here https://keras.io/models/sequential/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import **Dropout** to be used when adding layers: A Simple Way to Prevent Neural Networks from Overfitting\n",
    "\n",
    "Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting.\n",
    "\n",
    "Arguments:\n",
    "\n",
    "rate: float between 0 and 1. Fraction of the input units to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start by declaring a sequential model named classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the input layer and the first hidden layer using the .add() method\n",
    "\n",
    "Use the **Dense** method that takes the following arguments: \n",
    "\n",
    "**units**: dimensionality of the output space.\n",
    "\n",
    "**activation**: Activation function to use (relu for hidden layers, and sigmoid for output layer). \n",
    "\n",
    "**kernel_initializer**: Initializer for the kernel weights matrix.\n",
    "\n",
    "In addition, add the argument **input_dim**: dimension of the input layer, to be passed for the first hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply Dropout to the input units for regularization with a probability of 0.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dropout(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the second hidden layer made of 6 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 6, activation = 'relu',kernel_initializer = 'uniform'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the output layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 1, activation = 'sigmoid',kernel_initializer = 'uniform'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the ANN\n",
    "\n",
    "Before training a model, you need to configure the learning process, which is done via the **compile** method. \n",
    "\n",
    "It receives three arguments: an **optimizer** (use 'adam'), a **loss** function (use 'binary_crossentropy'), and list of **metrics** (use the 'accuracy'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss  = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the ANN to the Training set\n",
    "\n",
    "Keras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the **fit** function.\n",
    "\n",
    "Arguments: \n",
    "\n",
    "Input data, Target data,\n",
    "\n",
    "**batch_size**: Number of samples per gradient update \n",
    "\n",
    "**epochs**: Number of iterations on the dataset\n",
    "\n",
    "You are going to train your model using a batch size of 10, and 100 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7000/7000 [==============================] - 2s 218us/step - loss: 0.5457 - accuracy: 0.7986\n",
      "Epoch 2/100\n",
      "7000/7000 [==============================] - 1s 131us/step - loss: 0.5192 - accuracy: 0.8020\n",
      "Epoch 3/100\n",
      "7000/7000 [==============================] - 1s 131us/step - loss: 0.5144 - accuracy: 0.8020\n",
      "Epoch 4/100\n",
      "7000/7000 [==============================] - 1s 132us/step - loss: 0.5079 - accuracy: 0.8020\n",
      "Epoch 5/100\n",
      "7000/7000 [==============================] - 1s 139us/step - loss: 0.5020 - accuracy: 0.8020\n",
      "Epoch 6/100\n",
      "7000/7000 [==============================] - 1s 132us/step - loss: 0.4990 - accuracy: 0.8020\n",
      "Epoch 7/100\n",
      "7000/7000 [==============================] - 1s 134us/step - loss: 0.4960 - accuracy: 0.8020\n",
      "Epoch 8/100\n",
      "7000/7000 [==============================] - 1s 146us/step - loss: 0.4934 - accuracy: 0.8020\n",
      "Epoch 9/100\n",
      "7000/7000 [==============================] - 1s 187us/step - loss: 0.4937 - accuracy: 0.8020\n",
      "Epoch 10/100\n",
      "7000/7000 [==============================] - 1s 151us/step - loss: 0.4938 - accuracy: 0.8020\n",
      "Epoch 11/100\n",
      "7000/7000 [==============================] - 1s 149us/step - loss: 0.4945 - accuracy: 0.8020\n",
      "Epoch 12/100\n",
      "7000/7000 [==============================] - 1s 141us/step - loss: 0.4932 - accuracy: 0.8020\n",
      "Epoch 13/100\n",
      "7000/7000 [==============================] - 1s 147us/step - loss: 0.4944 - accuracy: 0.8020\n",
      "Epoch 14/100\n",
      "7000/7000 [==============================] - 1s 135us/step - loss: 0.4930 - accuracy: 0.8020\n",
      "Epoch 15/100\n",
      "7000/7000 [==============================] - 1s 139us/step - loss: 0.4955 - accuracy: 0.8020\n",
      "Epoch 16/100\n",
      "7000/7000 [==============================] - 1s 171us/step - loss: 0.4943 - accuracy: 0.8020\n",
      "Epoch 17/100\n",
      "7000/7000 [==============================] - 1s 157us/step - loss: 0.4943 - accuracy: 0.8020\n",
      "Epoch 18/100\n",
      "7000/7000 [==============================] - 1s 171us/step - loss: 0.4942 - accuracy: 0.8020\n",
      "Epoch 19/100\n",
      "7000/7000 [==============================] - 1s 143us/step - loss: 0.4936 - accuracy: 0.8020\n",
      "Epoch 20/100\n",
      "7000/7000 [==============================] - 1s 145us/step - loss: 0.4943 - accuracy: 0.8020\n",
      "Epoch 21/100\n",
      "7000/7000 [==============================] - 1s 178us/step - loss: 0.4960 - accuracy: 0.8020\n",
      "Epoch 22/100\n",
      "7000/7000 [==============================] - 1s 147us/step - loss: 0.4961 - accuracy: 0.8020\n",
      "Epoch 23/100\n",
      "7000/7000 [==============================] - 1s 172us/step - loss: 0.4955 - accuracy: 0.8020\n",
      "Epoch 24/100\n",
      "7000/7000 [==============================] - 1s 150us/step - loss: 0.4947 - accuracy: 0.8020\n",
      "Epoch 25/100\n",
      "7000/7000 [==============================] - 1s 133us/step - loss: 0.4940 - accuracy: 0.8020\n",
      "Epoch 26/100\n",
      "7000/7000 [==============================] - 1s 134us/step - loss: 0.4984 - accuracy: 0.8020\n",
      "Epoch 27/100\n",
      "7000/7000 [==============================] - 1s 125us/step - loss: 0.4981 - accuracy: 0.8020\n",
      "Epoch 28/100\n",
      "7000/7000 [==============================] - 1s 122us/step - loss: 0.4976 - accuracy: 0.8020\n",
      "Epoch 29/100\n",
      "7000/7000 [==============================] - 1s 132us/step - loss: 0.4975 - accuracy: 0.8020\n",
      "Epoch 30/100\n",
      "7000/7000 [==============================] - 1s 131us/step - loss: 0.4973 - accuracy: 0.8020\n",
      "Epoch 31/100\n",
      "7000/7000 [==============================] - 1s 126us/step - loss: 0.4971 - accuracy: 0.8020\n",
      "Epoch 32/100\n",
      "7000/7000 [==============================] - 1s 120us/step - loss: 0.4960 - accuracy: 0.8020\n",
      "Epoch 33/100\n",
      "7000/7000 [==============================] - 1s 126us/step - loss: 0.4935 - accuracy: 0.8020\n",
      "Epoch 34/100\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 0.4944 - accuracy: 0.8020\n",
      "Epoch 35/100\n",
      "7000/7000 [==============================] - 1s 170us/step - loss: 0.4944 - accuracy: 0.8020\n",
      "Epoch 36/100\n",
      "7000/7000 [==============================] - 1s 158us/step - loss: 0.4933 - accuracy: 0.8020\n",
      "Epoch 37/100\n",
      "7000/7000 [==============================] - 1s 171us/step - loss: 0.4932 - accuracy: 0.8020\n",
      "Epoch 38/100\n",
      "7000/7000 [==============================] - 1s 170us/step - loss: 0.4926 - accuracy: 0.8020\n",
      "Epoch 39/100\n",
      "7000/7000 [==============================] - 1s 146us/step - loss: 0.4927 - accuracy: 0.8020\n",
      "Epoch 40/100\n",
      "7000/7000 [==============================] - 1s 187us/step - loss: 0.4921 - accuracy: 0.8020\n",
      "Epoch 41/100\n",
      "7000/7000 [==============================] - 1s 136us/step - loss: 0.4917 - accuracy: 0.8020\n",
      "Epoch 42/100\n",
      "7000/7000 [==============================] - 1s 126us/step - loss: 0.4930 - accuracy: 0.8020\n",
      "Epoch 43/100\n",
      "7000/7000 [==============================] - 1s 142us/step - loss: 0.4936 - accuracy: 0.8020\n",
      "Epoch 44/100\n",
      "7000/7000 [==============================] - 1s 130us/step - loss: 0.4932 - accuracy: 0.8020\n",
      "Epoch 45/100\n",
      "7000/7000 [==============================] - 1s 132us/step - loss: 0.4925 - accuracy: 0.8020\n",
      "Epoch 46/100\n",
      "7000/7000 [==============================] - 1s 169us/step - loss: 0.4928 - accuracy: 0.8020\n",
      "Epoch 47/100\n",
      "7000/7000 [==============================] - 1s 154us/step - loss: 0.4918 - accuracy: 0.8020\n",
      "Epoch 48/100\n",
      "7000/7000 [==============================] - 1s 166us/step - loss: 0.4920 - accuracy: 0.8020\n",
      "Epoch 49/100\n",
      "7000/7000 [==============================] - 1s 167us/step - loss: 0.4909 - accuracy: 0.8020\n",
      "Epoch 50/100\n",
      "7000/7000 [==============================] - 1s 135us/step - loss: 0.4924 - accuracy: 0.8020\n",
      "Epoch 51/100\n",
      "7000/7000 [==============================] - 1s 138us/step - loss: 0.4907 - accuracy: 0.8020\n",
      "Epoch 52/100\n",
      "7000/7000 [==============================] - 1s 173us/step - loss: 0.4916 - accuracy: 0.8020\n",
      "Epoch 53/100\n",
      "7000/7000 [==============================] - 1s 156us/step - loss: 0.4913 - accuracy: 0.8020\n",
      "Epoch 54/100\n",
      "7000/7000 [==============================] - 1s 168us/step - loss: 0.4918 - accuracy: 0.8020\n",
      "Epoch 55/100\n",
      "7000/7000 [==============================] - 1s 187us/step - loss: 0.4921 - accuracy: 0.8020\n",
      "Epoch 56/100\n",
      "7000/7000 [==============================] - 1s 137us/step - loss: 0.4918 - accuracy: 0.8020\n",
      "Epoch 57/100\n",
      "7000/7000 [==============================] - 1s 134us/step - loss: 0.4909 - accuracy: 0.8020\n",
      "Epoch 58/100\n",
      "7000/7000 [==============================] - 1s 137us/step - loss: 0.4918 - accuracy: 0.8020\n",
      "Epoch 59/100\n",
      "7000/7000 [==============================] - 1s 135us/step - loss: 0.4919 - accuracy: 0.8020\n",
      "Epoch 60/100\n",
      "7000/7000 [==============================] - 1s 137us/step - loss: 0.4920 - accuracy: 0.8020\n",
      "Epoch 61/100\n",
      "7000/7000 [==============================] - 1s 135us/step - loss: 0.4913 - accuracy: 0.8020\n",
      "Epoch 62/100\n",
      "7000/7000 [==============================] - 1s 138us/step - loss: 0.4913 - accuracy: 0.8020\n",
      "Epoch 63/100\n",
      "7000/7000 [==============================] - 1s 135us/step - loss: 0.4919 - accuracy: 0.8020\n",
      "Epoch 64/100\n",
      "7000/7000 [==============================] - 1s 136us/step - loss: 0.4919 - accuracy: 0.8020\n",
      "Epoch 65/100\n",
      "7000/7000 [==============================] - 1s 147us/step - loss: 0.4916 - accuracy: 0.8020\n",
      "Epoch 66/100\n",
      "7000/7000 [==============================] - 1s 176us/step - loss: 0.4920 - accuracy: 0.8020\n",
      "Epoch 67/100\n",
      "7000/7000 [==============================] - 1s 195us/step - loss: 0.4914 - accuracy: 0.8020\n",
      "Epoch 68/100\n",
      "7000/7000 [==============================] - 1s 189us/step - loss: 0.4905 - accuracy: 0.8020\n",
      "Epoch 69/100\n",
      "7000/7000 [==============================] - 1s 163us/step - loss: 0.4906 - accuracy: 0.8020\n",
      "Epoch 70/100\n",
      "7000/7000 [==============================] - 1s 201us/step - loss: 0.4910 - accuracy: 0.8020\n",
      "Epoch 71/100\n",
      "7000/7000 [==============================] - 1s 179us/step - loss: 0.4906 - accuracy: 0.8020\n",
      "Epoch 72/100\n",
      "7000/7000 [==============================] - 1s 176us/step - loss: 0.4908 - accuracy: 0.8020\n",
      "Epoch 73/100\n",
      "7000/7000 [==============================] - 1s 177us/step - loss: 0.4909 - accuracy: 0.8020\n",
      "Epoch 74/100\n",
      "7000/7000 [==============================] - 1s 181us/step - loss: 0.4909 - accuracy: 0.8020\n",
      "Epoch 75/100\n",
      "7000/7000 [==============================] - 1s 156us/step - loss: 0.4910 - accuracy: 0.8020\n",
      "Epoch 76/100\n",
      "7000/7000 [==============================] - 1s 164us/step - loss: 0.4912 - accuracy: 0.8020\n",
      "Epoch 77/100\n",
      "7000/7000 [==============================] - 1s 169us/step - loss: 0.4909 - accuracy: 0.8020\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 1s 139us/step - loss: 0.4910 - accuracy: 0.8020\n",
      "Epoch 79/100\n",
      "7000/7000 [==============================] - 1s 130us/step - loss: 0.4909 - accuracy: 0.8020\n",
      "Epoch 80/100\n",
      "7000/7000 [==============================] - 1s 130us/step - loss: 0.4906 - accuracy: 0.8020\n",
      "Epoch 81/100\n",
      "7000/7000 [==============================] - 1s 128us/step - loss: 0.4908 - accuracy: 0.8020\n",
      "Epoch 82/100\n",
      "7000/7000 [==============================] - 1s 127us/step - loss: 0.4910 - accuracy: 0.8020\n",
      "Epoch 83/100\n",
      "7000/7000 [==============================] - 1s 129us/step - loss: 0.4908 - accuracy: 0.8020\n",
      "Epoch 84/100\n",
      "7000/7000 [==============================] - 1s 158us/step - loss: 0.4909 - accuracy: 0.8020\n",
      "Epoch 85/100\n",
      "7000/7000 [==============================] - 1s 148us/step - loss: 0.4905 - accuracy: 0.8020\n",
      "Epoch 86/100\n",
      "7000/7000 [==============================] - 1s 127us/step - loss: 0.4912 - accuracy: 0.8020\n",
      "Epoch 87/100\n",
      "7000/7000 [==============================] - 1s 129us/step - loss: 0.4911 - accuracy: 0.8020\n",
      "Epoch 88/100\n",
      "7000/7000 [==============================] - 1s 127us/step - loss: 0.4910 - accuracy: 0.8020\n",
      "Epoch 89/100\n",
      "7000/7000 [==============================] - 1s 132us/step - loss: 0.4909 - accuracy: 0.8020\n",
      "Epoch 90/100\n",
      "7000/7000 [==============================] - 1s 127us/step - loss: 0.4910 - accuracy: 0.8020\n",
      "Epoch 91/100\n",
      "7000/7000 [==============================] - 1s 139us/step - loss: 0.4909 - accuracy: 0.8020\n",
      "Epoch 92/100\n",
      "7000/7000 [==============================] - 1s 132us/step - loss: 0.4908 - accuracy: 0.8020\n",
      "Epoch 93/100\n",
      "7000/7000 [==============================] - 1s 131us/step - loss: 0.4911 - accuracy: 0.8020\n",
      "Epoch 94/100\n",
      "7000/7000 [==============================] - 1s 129us/step - loss: 0.4909 - accuracy: 0.8020\n",
      "Epoch 95/100\n",
      "7000/7000 [==============================] - 1s 129us/step - loss: 0.4905 - accuracy: 0.8020\n",
      "Epoch 96/100\n",
      "7000/7000 [==============================] - 1s 128us/step - loss: 0.4908 - accuracy: 0.8020\n",
      "Epoch 97/100\n",
      "7000/7000 [==============================] - 1s 137us/step - loss: 0.4910 - accuracy: 0.8020\n",
      "Epoch 98/100\n",
      "7000/7000 [==============================] - 1s 126us/step - loss: 0.4908 - accuracy: 0.8020\n",
      "Epoch 99/100\n",
      "7000/7000 [==============================] - 1s 127us/step - loss: 0.4910 - accuracy: 0.8020\n",
      "Epoch 100/100\n",
      "7000/7000 [==============================] - 1s 127us/step - loss: 0.4907 - accuracy: 0.8020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1f8b712dd08>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train,y_train,batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Making predictions and evaluating the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the Test set results\n",
    "\n",
    "#### Use the .predict(input data) method. It generates output predictions for the input samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24532558],\n",
       "       [0.24532558],\n",
       "       [0.1386362 ],\n",
       "       ...,\n",
       "       [0.1386362 ],\n",
       "       [0.24532558],\n",
       "       [0.24532558]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = classifier.predict(X_test)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the output value of a sigmoid function varies between 0 and 1. Choose a threshold (use 0.5) to assign a value of 1 for predictions higher than 0.5, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       ...,\n",
       "       [False],\n",
       "       [False],\n",
       "       [False]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = (res > 0.5)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2349,    0],\n",
       "       [ 651,    0]], dtype=int64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, res)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting a single new observation\n",
    "\n",
    "Predict if the customer with the following informations will leave the bank:\n",
    "Geography : France  -  Credit score : 600  -  Gender : Male  -  Age : 40  -  Tenure : 3  -  Balance : 60000  -  Number of products : 2  -  Has credit card: Yes  -  Is an active memeber: Yes  -  Estimated salary : 50000\n",
    "\n",
    "What is the output value of your classifier? is the customer going to leave the bank?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred = classifier.predict(sc.transform(np.array([[0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])))\n",
    "new_pred = (new_pred > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4 - Evaluating the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use Sequential Keras models as part of your Scikit-Learn workflow via the wrappers found in keras library. More info can be found here https://keras.io/scikit-learn-api/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KerasClassifier Arguments :\n",
    "    \n",
    "build_fn should construct, compile and return a Keras model, which will then be used to fit/predict. \n",
    "\n",
    "sk_params: model parameters & fitting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create the function that builds the architecture of the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier  #classifier is local to this function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an object inside this class named kclassifier that takes the previous function as arguments, and  the batch_size and the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "kclassifier = KerasClassifier(build_fn = build_classifier, batch_size = 10, epochs = 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the cross_val_score function. Define an array that will contain the k-accuracies of the k-fold CV, name it accuracies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4666/4666 [==============================] - 1s 269us/step - loss: 0.5631 - accuracy: 0.7960\n",
      "Epoch 2/100\n",
      "4666/4666 [==============================] - 1s 149us/step - loss: 0.5297 - accuracy: 0.7998\n",
      "Epoch 3/100\n",
      "4666/4666 [==============================] - 1s 173us/step - loss: 0.5225 - accuracy: 0.7998\n",
      "Epoch 4/100\n",
      "4666/4666 [==============================] - 1s 159us/step - loss: 0.5195 - accuracy: 0.7998\n",
      "Epoch 5/100\n",
      "4666/4666 [==============================] - 1s 156us/step - loss: 0.5154 - accuracy: 0.7998\n",
      "Epoch 6/100\n",
      "4666/4666 [==============================] - 1s 152us/step - loss: 0.5095 - accuracy: 0.7998\n",
      "Epoch 7/100\n",
      "4666/4666 [==============================] - 1s 153us/step - loss: 0.5076 - accuracy: 0.7998\n",
      "Epoch 8/100\n",
      "4666/4666 [==============================] - 1s 156us/step - loss: 0.5035 - accuracy: 0.7998\n",
      "Epoch 9/100\n",
      "4666/4666 [==============================] - 1s 173us/step - loss: 0.4993 - accuracy: 0.7998\n",
      "Epoch 10/100\n",
      "4666/4666 [==============================] - 1s 167us/step - loss: 0.4984 - accuracy: 0.7998\n",
      "Epoch 11/100\n",
      "4666/4666 [==============================] - 1s 170us/step - loss: 0.4982 - accuracy: 0.7998\n",
      "Epoch 12/100\n",
      "4666/4666 [==============================] - 1s 165us/step - loss: 0.4964 - accuracy: 0.7998\n",
      "Epoch 13/100\n",
      "4666/4666 [==============================] - 1s 151us/step - loss: 0.4960 - accuracy: 0.7998\n",
      "Epoch 14/100\n",
      "4666/4666 [==============================] - 1s 161us/step - loss: 0.4983 - accuracy: 0.7998\n",
      "Epoch 15/100\n",
      "4666/4666 [==============================] - 1s 202us/step - loss: 0.4959 - accuracy: 0.7998\n",
      "Epoch 16/100\n",
      "4666/4666 [==============================] - 1s 181us/step - loss: 0.4993 - accuracy: 0.7998\n",
      "Epoch 17/100\n",
      "4666/4666 [==============================] - 1s 158us/step - loss: 0.4972 - accuracy: 0.7998\n",
      "Epoch 18/100\n",
      "4666/4666 [==============================] - 1s 139us/step - loss: 0.4960 - accuracy: 0.7998\n",
      "Epoch 19/100\n",
      "4666/4666 [==============================] - 1s 156us/step - loss: 0.4977 - accuracy: 0.7998\n",
      "Epoch 20/100\n",
      "4666/4666 [==============================] - 1s 148us/step - loss: 0.4980 - accuracy: 0.7998\n",
      "Epoch 21/100\n",
      "4666/4666 [==============================] - 1s 141us/step - loss: 0.4956 - accuracy: 0.7998\n",
      "Epoch 22/100\n",
      "4666/4666 [==============================] - 1s 152us/step - loss: 0.4949 - accuracy: 0.7998\n",
      "Epoch 23/100\n",
      "4666/4666 [==============================] - 1s 134us/step - loss: 0.4960 - accuracy: 0.7998\n",
      "Epoch 24/100\n",
      "4666/4666 [==============================] - 1s 163us/step - loss: 0.4983 - accuracy: 0.7998\n",
      "Epoch 25/100\n",
      "4666/4666 [==============================] - 1s 128us/step - loss: 0.4970 - accuracy: 0.7998\n",
      "Epoch 26/100\n",
      "4666/4666 [==============================] - 1s 129us/step - loss: 0.4959 - accuracy: 0.7998\n",
      "Epoch 27/100\n",
      "4666/4666 [==============================] - 1s 128us/step - loss: 0.4957 - accuracy: 0.7998\n",
      "Epoch 28/100\n",
      "4666/4666 [==============================] - 1s 129us/step - loss: 0.4966 - accuracy: 0.7998\n",
      "Epoch 29/100\n",
      "4666/4666 [==============================] - 1s 130us/step - loss: 0.4952 - accuracy: 0.7998\n",
      "Epoch 30/100\n",
      "4666/4666 [==============================] - 1s 128us/step - loss: 0.4962 - accuracy: 0.7998\n",
      "Epoch 31/100\n",
      "4666/4666 [==============================] - 1s 128us/step - loss: 0.4961 - accuracy: 0.7998\n",
      "Epoch 32/100\n",
      "4666/4666 [==============================] - 1s 128us/step - loss: 0.4977 - accuracy: 0.7998\n",
      "Epoch 33/100\n",
      "4666/4666 [==============================] - 1s 129us/step - loss: 0.4964 - accuracy: 0.7998\n",
      "Epoch 34/100\n",
      "4666/4666 [==============================] - 1s 128us/step - loss: 0.4966 - accuracy: 0.7998\n",
      "Epoch 35/100\n",
      "4666/4666 [==============================] - 1s 128us/step - loss: 0.4964 - accuracy: 0.7998\n",
      "Epoch 36/100\n",
      "4666/4666 [==============================] - 1s 130us/step - loss: 0.4949 - accuracy: 0.7998\n",
      "Epoch 37/100\n",
      "4666/4666 [==============================] - 1s 130us/step - loss: 0.4957 - accuracy: 0.7998\n",
      "Epoch 38/100\n",
      "4666/4666 [==============================] - 1s 131us/step - loss: 0.4961 - accuracy: 0.7998\n",
      "Epoch 39/100\n",
      "4666/4666 [==============================] - 1s 186us/step - loss: 0.4954 - accuracy: 0.7998\n",
      "Epoch 40/100\n",
      "4666/4666 [==============================] - 1s 148us/step - loss: 0.4949 - accuracy: 0.7998\n",
      "Epoch 41/100\n",
      "4666/4666 [==============================] - 1s 131us/step - loss: 0.4953 - accuracy: 0.7998\n",
      "Epoch 42/100\n",
      "4666/4666 [==============================] - 1s 138us/step - loss: 0.4951 - accuracy: 0.7998\n",
      "Epoch 43/100\n",
      "4666/4666 [==============================] - 1s 150us/step - loss: 0.4972 - accuracy: 0.7998\n",
      "Epoch 44/100\n",
      "4666/4666 [==============================] - 1s 146us/step - loss: 0.4954 - accuracy: 0.7998\n",
      "Epoch 45/100\n",
      "4666/4666 [==============================] - 1s 139us/step - loss: 0.4959 - accuracy: 0.7998\n",
      "Epoch 46/100\n",
      "4666/4666 [==============================] - 1s 138us/step - loss: 0.4959 - accuracy: 0.7998\n",
      "Epoch 47/100\n",
      "4666/4666 [==============================] - 1s 132us/step - loss: 0.4963 - accuracy: 0.7998\n",
      "Epoch 48/100\n",
      "4666/4666 [==============================] - 1s 132us/step - loss: 0.4950 - accuracy: 0.7998\n",
      "Epoch 49/100\n",
      "4666/4666 [==============================] - 1s 131us/step - loss: 0.4955 - accuracy: 0.7998\n",
      "Epoch 50/100\n",
      "4666/4666 [==============================] - 1s 127us/step - loss: 0.4946 - accuracy: 0.7998\n",
      "Epoch 51/100\n",
      "4666/4666 [==============================] - 1s 130us/step - loss: 0.4962 - accuracy: 0.7998\n",
      "Epoch 52/100\n",
      "4666/4666 [==============================] - 1s 128us/step - loss: 0.4960 - accuracy: 0.7998\n",
      "Epoch 53/100\n",
      "4666/4666 [==============================] - 1s 131us/step - loss: 0.4943 - accuracy: 0.7998\n",
      "Epoch 54/100\n",
      "4666/4666 [==============================] - 1s 128us/step - loss: 0.4956 - accuracy: 0.7998\n",
      "Epoch 55/100\n",
      "4666/4666 [==============================] - 1s 128us/step - loss: 0.4974 - accuracy: 0.7998\n",
      "Epoch 56/100\n",
      "4666/4666 [==============================] - 1s 129us/step - loss: 0.4956 - accuracy: 0.7998\n",
      "Epoch 57/100\n",
      "4666/4666 [==============================] - 1s 127us/step - loss: 0.4960 - accuracy: 0.7998\n",
      "Epoch 58/100\n",
      "4666/4666 [==============================] - 1s 127us/step - loss: 0.4972 - accuracy: 0.7998\n",
      "Epoch 59/100\n",
      "4666/4666 [==============================] - 1s 128us/step - loss: 0.4953 - accuracy: 0.79980s - loss: 0.490\n",
      "Epoch 60/100\n",
      "4666/4666 [==============================] - 1s 125us/step - loss: 0.4963 - accuracy: 0.7998\n",
      "Epoch 61/100\n",
      "4666/4666 [==============================] - 1s 128us/step - loss: 0.4958 - accuracy: 0.7998\n",
      "Epoch 62/100\n",
      "4666/4666 [==============================] - 1s 129us/step - loss: 0.4957 - accuracy: 0.7998\n",
      "Epoch 63/100\n",
      "4666/4666 [==============================] - 1s 127us/step - loss: 0.4955 - accuracy: 0.7998\n",
      "Epoch 64/100\n",
      "4666/4666 [==============================] - 1s 169us/step - loss: 0.4961 - accuracy: 0.7998\n",
      "Epoch 65/100\n",
      "4666/4666 [==============================] - 1s 166us/step - loss: 0.4947 - accuracy: 0.7998\n",
      "Epoch 66/100\n",
      "4666/4666 [==============================] - 1s 125us/step - loss: 0.4959 - accuracy: 0.7998\n",
      "Epoch 67/100\n",
      "4666/4666 [==============================] - 1s 129us/step - loss: 0.4965 - accuracy: 0.7998\n",
      "Epoch 68/100\n",
      "4666/4666 [==============================] - 1s 126us/step - loss: 0.4954 - accuracy: 0.7998\n",
      "Epoch 69/100\n",
      "4666/4666 [==============================] - 1s 129us/step - loss: 0.4952 - accuracy: 0.7998\n",
      "Epoch 70/100\n",
      "4666/4666 [==============================] - 1s 129us/step - loss: 0.4952 - accuracy: 0.7998\n",
      "Epoch 71/100\n",
      "4666/4666 [==============================] - 1s 129us/step - loss: 0.4949 - accuracy: 0.7998\n",
      "Epoch 72/100\n",
      "4666/4666 [==============================] - 1s 130us/step - loss: 0.4957 - accuracy: 0.7998\n",
      "Epoch 73/100\n",
      "4666/4666 [==============================] - 1s 128us/step - loss: 0.4951 - accuracy: 0.7998\n",
      "Epoch 74/100\n",
      "4666/4666 [==============================] - 1s 130us/step - loss: 0.4951 - accuracy: 0.7998\n",
      "Epoch 75/100\n",
      "4666/4666 [==============================] - 1s 128us/step - loss: 0.4957 - accuracy: 0.7998\n",
      "Epoch 76/100\n",
      "4666/4666 [==============================] - 1s 128us/step - loss: 0.4946 - accuracy: 0.7998\n",
      "Epoch 77/100\n",
      "4666/4666 [==============================] - 1s 130us/step - loss: 0.4950 - accuracy: 0.7998\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4666/4666 [==============================] - 1s 121us/step - loss: 0.4945 - accuracy: 0.7998\n",
      "Epoch 79/100\n",
      "4666/4666 [==============================] - 1s 118us/step - loss: 0.4944 - accuracy: 0.7998\n",
      "Epoch 80/100\n",
      "4666/4666 [==============================] - 1s 119us/step - loss: 0.4945 - accuracy: 0.7998\n",
      "Epoch 81/100\n",
      "4666/4666 [==============================] - 1s 119us/step - loss: 0.4953 - accuracy: 0.7998\n",
      "Epoch 82/100\n",
      "4666/4666 [==============================] - 1s 120us/step - loss: 0.4939 - accuracy: 0.7998\n",
      "Epoch 83/100\n",
      "4666/4666 [==============================] - 1s 118us/step - loss: 0.4949 - accuracy: 0.7998\n",
      "Epoch 84/100\n",
      "4666/4666 [==============================] - 1s 120us/step - loss: 0.4950 - accuracy: 0.7998\n",
      "Epoch 85/100\n",
      "4666/4666 [==============================] - 1s 119us/step - loss: 0.4940 - accuracy: 0.7998\n",
      "Epoch 86/100\n",
      "4666/4666 [==============================] - 1s 128us/step - loss: 0.4950 - accuracy: 0.7998\n",
      "Epoch 87/100\n",
      "4666/4666 [==============================] - 1s 119us/step - loss: 0.4946 - accuracy: 0.7998\n",
      "Epoch 88/100\n",
      "4666/4666 [==============================] - 1s 122us/step - loss: 0.4941 - accuracy: 0.7998\n",
      "Epoch 89/100\n",
      "4666/4666 [==============================] - 1s 129us/step - loss: 0.4940 - accuracy: 0.7998\n",
      "Epoch 90/100\n",
      "4666/4666 [==============================] - 1s 121us/step - loss: 0.4950 - accuracy: 0.7998\n",
      "Epoch 91/100\n",
      "4666/4666 [==============================] - 1s 170us/step - loss: 0.4945 - accuracy: 0.7998\n",
      "Epoch 92/100\n",
      "4666/4666 [==============================] - 1s 164us/step - loss: 0.4949 - accuracy: 0.7998\n",
      "Epoch 93/100\n",
      "4666/4666 [==============================] - 1s 135us/step - loss: 0.4965 - accuracy: 0.7998\n",
      "Epoch 94/100\n",
      "4666/4666 [==============================] - 1s 130us/step - loss: 0.4954 - accuracy: 0.7998\n",
      "Epoch 95/100\n",
      "4666/4666 [==============================] - 1s 130us/step - loss: 0.4950 - accuracy: 0.7998\n",
      "Epoch 96/100\n",
      "4666/4666 [==============================] - 1s 135us/step - loss: 0.4949 - accuracy: 0.7998\n",
      "Epoch 97/100\n",
      "4666/4666 [==============================] - 1s 128us/step - loss: 0.4960 - accuracy: 0.7998\n",
      "Epoch 98/100\n",
      "4666/4666 [==============================] - 1s 127us/step - loss: 0.4950 - accuracy: 0.7998\n",
      "Epoch 99/100\n",
      "4666/4666 [==============================] - 1s 120us/step - loss: 0.4955 - accuracy: 0.7998\n",
      "Epoch 100/100\n",
      "4666/4666 [==============================] - 1s 119us/step - loss: 0.4952 - accuracy: 0.7998\n",
      "2334/2334 [==============================] - 0s 72us/step\n",
      "Epoch 1/100\n",
      "4667/4667 [==============================] - 1s 199us/step - loss: 0.5392 - accuracy: 0.8059\n",
      "Epoch 2/100\n",
      "4667/4667 [==============================] - 1s 122us/step - loss: 0.5209 - accuracy: 0.8065\n",
      "Epoch 3/100\n",
      "4667/4667 [==============================] - 1s 121us/step - loss: 0.5083 - accuracy: 0.8065\n",
      "Epoch 4/100\n",
      "4667/4667 [==============================] - 1s 125us/step - loss: 0.5032 - accuracy: 0.8065\n",
      "Epoch 5/100\n",
      "4667/4667 [==============================] - 1s 123us/step - loss: 0.4988 - accuracy: 0.8065\n",
      "Epoch 6/100\n",
      "4667/4667 [==============================] - 1s 122us/step - loss: 0.5052 - accuracy: 0.8065\n",
      "Epoch 7/100\n",
      "4667/4667 [==============================] - 1s 124us/step - loss: 0.4960 - accuracy: 0.8065\n",
      "Epoch 8/100\n",
      "4667/4667 [==============================] - 1s 121us/step - loss: 0.4919 - accuracy: 0.8065\n",
      "Epoch 9/100\n",
      "4667/4667 [==============================] - 1s 123us/step - loss: 0.4918 - accuracy: 0.8065\n",
      "Epoch 10/100\n",
      "4667/4667 [==============================] - 1s 124us/step - loss: 0.4917 - accuracy: 0.8065\n",
      "Epoch 11/100\n",
      "4667/4667 [==============================] - 1s 124us/step - loss: 0.4894 - accuracy: 0.8065\n",
      "Epoch 12/100\n",
      "4667/4667 [==============================] - 1s 122us/step - loss: 0.4888 - accuracy: 0.8065\n",
      "Epoch 13/100\n",
      "4667/4667 [==============================] - 1s 126us/step - loss: 0.4870 - accuracy: 0.8065\n",
      "Epoch 14/100\n",
      "4667/4667 [==============================] - 1s 120us/step - loss: 0.4857 - accuracy: 0.8065\n",
      "Epoch 15/100\n",
      "4667/4667 [==============================] - 1s 129us/step - loss: 0.4863 - accuracy: 0.8065\n",
      "Epoch 16/100\n",
      "4667/4667 [==============================] - 1s 181us/step - loss: 0.4874 - accuracy: 0.8065\n",
      "Epoch 17/100\n",
      "4667/4667 [==============================] - 1s 141us/step - loss: 0.4865 - accuracy: 0.8065\n",
      "Epoch 18/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4861 - accuracy: 0.8065\n",
      "Epoch 19/100\n",
      "4667/4667 [==============================] - 1s 120us/step - loss: 0.4944 - accuracy: 0.8065\n",
      "Epoch 20/100\n",
      "4667/4667 [==============================] - 1s 118us/step - loss: 0.4909 - accuracy: 0.8065\n",
      "Epoch 21/100\n",
      "4667/4667 [==============================] - 1s 122us/step - loss: 0.4864 - accuracy: 0.8065\n",
      "Epoch 22/100\n",
      "4667/4667 [==============================] - 1s 118us/step - loss: 0.4862 - accuracy: 0.8065\n",
      "Epoch 23/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4869 - accuracy: 0.8065\n",
      "Epoch 24/100\n",
      "4667/4667 [==============================] - 1s 127us/step - loss: 0.4850 - accuracy: 0.8065\n",
      "Epoch 25/100\n",
      "4667/4667 [==============================] - 1s 156us/step - loss: 0.4865 - accuracy: 0.8065\n",
      "Epoch 26/100\n",
      "4667/4667 [==============================] - 1s 120us/step - loss: 0.4856 - accuracy: 0.8065\n",
      "Epoch 27/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4868 - accuracy: 0.8065\n",
      "Epoch 28/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4855 - accuracy: 0.8065\n",
      "Epoch 29/100\n",
      "4667/4667 [==============================] - 1s 120us/step - loss: 0.4855 - accuracy: 0.8065\n",
      "Epoch 30/100\n",
      "4667/4667 [==============================] - 1s 118us/step - loss: 0.4843 - accuracy: 0.8065\n",
      "Epoch 31/100\n",
      "4667/4667 [==============================] - 1s 120us/step - loss: 0.4846 - accuracy: 0.8065\n",
      "Epoch 32/100\n",
      "4667/4667 [==============================] - 1s 118us/step - loss: 0.4845 - accuracy: 0.8065\n",
      "Epoch 33/100\n",
      "4667/4667 [==============================] - 1s 120us/step - loss: 0.4848 - accuracy: 0.8065\n",
      "Epoch 34/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4844 - accuracy: 0.8065\n",
      "Epoch 35/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4836 - accuracy: 0.8065\n",
      "Epoch 36/100\n",
      "4667/4667 [==============================] - 1s 120us/step - loss: 0.4855 - accuracy: 0.8065\n",
      "Epoch 37/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4848 - accuracy: 0.8065\n",
      "Epoch 38/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4866 - accuracy: 0.8065\n",
      "Epoch 39/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4853 - accuracy: 0.8065\n",
      "Epoch 40/100\n",
      "4667/4667 [==============================] - 1s 121us/step - loss: 0.4843 - accuracy: 0.8065\n",
      "Epoch 41/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4845 - accuracy: 0.8065\n",
      "Epoch 42/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4877 - accuracy: 0.8065\n",
      "Epoch 43/100\n",
      "4667/4667 [==============================] - 1s 156us/step - loss: 0.4906 - accuracy: 0.8065\n",
      "Epoch 44/100\n",
      "4667/4667 [==============================] - 1s 160us/step - loss: 0.4903 - accuracy: 0.8065\n",
      "Epoch 45/100\n",
      "4667/4667 [==============================] - 1s 122us/step - loss: 0.4903 - accuracy: 0.8065\n",
      "Epoch 46/100\n",
      "4667/4667 [==============================] - 1s 120us/step - loss: 0.4903 - accuracy: 0.8065\n",
      "Epoch 47/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4903 - accuracy: 0.8065\n",
      "Epoch 48/100\n",
      "4667/4667 [==============================] - 1s 123us/step - loss: 0.4902 - accuracy: 0.8065\n",
      "Epoch 49/100\n",
      "4667/4667 [==============================] - 1s 121us/step - loss: 0.4894 - accuracy: 0.8065\n",
      "Epoch 50/100\n",
      "4667/4667 [==============================] - 1s 142us/step - loss: 0.4876 - accuracy: 0.8065\n",
      "Epoch 51/100\n",
      "4667/4667 [==============================] - 1s 138us/step - loss: 0.4859 - accuracy: 0.8065\n",
      "Epoch 52/100\n",
      "4667/4667 [==============================] - 1s 131us/step - loss: 0.4853 - accuracy: 0.8065\n",
      "Epoch 53/100\n",
      "4667/4667 [==============================] - 1s 129us/step - loss: 0.4848 - accuracy: 0.8065\n",
      "Epoch 54/100\n",
      "4667/4667 [==============================] - 1s 130us/step - loss: 0.4844 - accuracy: 0.8065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "4667/4667 [==============================] - 1s 127us/step - loss: 0.4841 - accuracy: 0.8065\n",
      "Epoch 56/100\n",
      "4667/4667 [==============================] - 1s 123us/step - loss: 0.4838 - accuracy: 0.8065\n",
      "Epoch 57/100\n",
      "4667/4667 [==============================] - 1s 120us/step - loss: 0.4837 - accuracy: 0.8065\n",
      "Epoch 58/100\n",
      "4667/4667 [==============================] - 1s 118us/step - loss: 0.4835 - accuracy: 0.8065\n",
      "Epoch 59/100\n",
      "4667/4667 [==============================] - 1s 116us/step - loss: 0.4834 - accuracy: 0.8065\n",
      "Epoch 60/100\n",
      "4667/4667 [==============================] - 1s 118us/step - loss: 0.4833 - accuracy: 0.8065\n",
      "Epoch 61/100\n",
      "4667/4667 [==============================] - 1s 117us/step - loss: 0.4832 - accuracy: 0.8065\n",
      "Epoch 62/100\n",
      "4667/4667 [==============================] - 1s 120us/step - loss: 0.4832 - accuracy: 0.8065\n",
      "Epoch 63/100\n",
      "4667/4667 [==============================] - 1s 117us/step - loss: 0.4829 - accuracy: 0.8065\n",
      "Epoch 64/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4832 - accuracy: 0.8065\n",
      "Epoch 65/100\n",
      "4667/4667 [==============================] - 1s 118us/step - loss: 0.4831 - accuracy: 0.8065\n",
      "Epoch 66/100\n",
      "4667/4667 [==============================] - 1s 118us/step - loss: 0.4832 - accuracy: 0.8065\n",
      "Epoch 67/100\n",
      "4667/4667 [==============================] - 1s 117us/step - loss: 0.4830 - accuracy: 0.8065\n",
      "Epoch 68/100\n",
      "4667/4667 [==============================] - 1s 117us/step - loss: 0.4831 - accuracy: 0.8065\n",
      "Epoch 69/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4830 - accuracy: 0.8065\n",
      "Epoch 70/100\n",
      "4667/4667 [==============================] - 1s 135us/step - loss: 0.4830 - accuracy: 0.8065\n",
      "Epoch 71/100\n",
      "4667/4667 [==============================] - 1s 173us/step - loss: 0.4829 - accuracy: 0.8065\n",
      "Epoch 72/100\n",
      "4667/4667 [==============================] - 1s 126us/step - loss: 0.4831 - accuracy: 0.8065\n",
      "Epoch 73/100\n",
      "4667/4667 [==============================] - 1s 118us/step - loss: 0.4831 - accuracy: 0.8065\n",
      "Epoch 74/100\n",
      "4667/4667 [==============================] - 1s 120us/step - loss: 0.4831 - accuracy: 0.8065\n",
      "Epoch 75/100\n",
      "4667/4667 [==============================] - 1s 118us/step - loss: 0.4830 - accuracy: 0.8065\n",
      "Epoch 76/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4830 - accuracy: 0.8065\n",
      "Epoch 77/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4830 - accuracy: 0.8065\n",
      "Epoch 78/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4830 - accuracy: 0.8065\n",
      "Epoch 79/100\n",
      "4667/4667 [==============================] - 1s 118us/step - loss: 0.4830 - accuracy: 0.8065\n",
      "Epoch 80/100\n",
      "4667/4667 [==============================] - 1s 118us/step - loss: 0.4830 - accuracy: 0.8065\n",
      "Epoch 81/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4830 - accuracy: 0.8065\n",
      "Epoch 82/100\n",
      "4667/4667 [==============================] - 1s 118us/step - loss: 0.4831 - accuracy: 0.8065\n",
      "Epoch 83/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4830 - accuracy: 0.8065\n",
      "Epoch 84/100\n",
      "4667/4667 [==============================] - 1s 118us/step - loss: 0.4830 - accuracy: 0.8065\n",
      "Epoch 85/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4831 - accuracy: 0.8065\n",
      "Epoch 86/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4830 - accuracy: 0.8065\n",
      "Epoch 87/100\n",
      "4667/4667 [==============================] - 1s 127us/step - loss: 0.4830 - accuracy: 0.8065\n",
      "Epoch 88/100\n",
      "4667/4667 [==============================] - 1s 118us/step - loss: 0.4830 - accuracy: 0.8065\n",
      "Epoch 89/100\n",
      "4667/4667 [==============================] - 1s 118us/step - loss: 0.4831 - accuracy: 0.8065\n",
      "Epoch 90/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4830 - accuracy: 0.8065\n",
      "Epoch 91/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4830 - accuracy: 0.8065\n",
      "Epoch 92/100\n",
      "4667/4667 [==============================] - 1s 120us/step - loss: 0.4830 - accuracy: 0.8065\n",
      "Epoch 93/100\n",
      "4667/4667 [==============================] - 1s 120us/step - loss: 0.4830 - accuracy: 0.8065\n",
      "Epoch 94/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4830 - accuracy: 0.8065\n",
      "Epoch 95/100\n",
      "4667/4667 [==============================] - 1s 120us/step - loss: 0.4831 - accuracy: 0.8065\n",
      "Epoch 96/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4830 - accuracy: 0.8065\n",
      "Epoch 97/100\n",
      "4667/4667 [==============================] - 1s 119us/step - loss: 0.4829 - accuracy: 0.8065\n",
      "Epoch 98/100\n",
      "4667/4667 [==============================] - 1s 133us/step - loss: 0.4830 - accuracy: 0.8065\n",
      "Epoch 99/100\n",
      "4667/4667 [==============================] - 1s 174us/step - loss: 0.4829 - accuracy: 0.8065\n",
      "Epoch 100/100\n",
      "4667/4667 [==============================] - 1s 129us/step - loss: 0.4830 - accuracy: 0.8065\n",
      "2333/2333 [==============================] - 0s 71us/step\n",
      "Epoch 1/100\n",
      "4667/4667 [==============================] - 1s 230us/step - loss: 0.6395 - accuracy: 0.7969\n",
      "Epoch 2/100\n",
      "4667/4667 [==============================] - 1s 132us/step - loss: 0.5248 - accuracy: 0.7997\n",
      "Epoch 3/100\n",
      "4667/4667 [==============================] - 1s 145us/step - loss: 0.5203 - accuracy: 0.7997\n",
      "Epoch 4/100\n",
      "4667/4667 [==============================] - 1s 198us/step - loss: 0.5143 - accuracy: 0.7997\n",
      "Epoch 5/100\n",
      "4667/4667 [==============================] - 1s 154us/step - loss: 0.5117 - accuracy: 0.7997\n",
      "Epoch 6/100\n",
      "4667/4667 [==============================] - 1s 132us/step - loss: 0.5060 - accuracy: 0.7997\n",
      "Epoch 7/100\n",
      "4667/4667 [==============================] - 1s 127us/step - loss: 0.5044 - accuracy: 0.7997\n",
      "Epoch 8/100\n",
      "4667/4667 [==============================] - 1s 131us/step - loss: 0.5012 - accuracy: 0.7997\n",
      "Epoch 9/100\n",
      "4667/4667 [==============================] - 1s 132us/step - loss: 0.5007 - accuracy: 0.7997\n",
      "Epoch 10/100\n",
      "4667/4667 [==============================] - 1s 151us/step - loss: 0.4992 - accuracy: 0.7997\n",
      "Epoch 11/100\n",
      "4667/4667 [==============================] - 1s 274us/step - loss: 0.4984 - accuracy: 0.7997\n",
      "Epoch 12/100\n",
      "4667/4667 [==============================] - 1s 203us/step - loss: 0.4984 - accuracy: 0.7997\n",
      "Epoch 13/100\n",
      "4667/4667 [==============================] - 1s 231us/step - loss: 0.5006 - accuracy: 0.7997\n",
      "Epoch 14/100\n",
      "4667/4667 [==============================] - 1s 207us/step - loss: 0.4993 - accuracy: 0.7997\n",
      "Epoch 15/100\n",
      "4667/4667 [==============================] - 1s 232us/step - loss: 0.5004 - accuracy: 0.7997\n",
      "Epoch 16/100\n",
      "4667/4667 [==============================] - 1s 183us/step - loss: 0.5000 - accuracy: 0.7997\n",
      "Epoch 17/100\n",
      "4667/4667 [==============================] - 1s 302us/step - loss: 0.5001 - accuracy: 0.7997\n",
      "Epoch 18/100\n",
      "4667/4667 [==============================] - 1s 275us/step - loss: 0.4999 - accuracy: 0.7997\n",
      "Epoch 19/100\n",
      "4667/4667 [==============================] - 1s 276us/step - loss: 0.5005 - accuracy: 0.7997\n",
      "Epoch 20/100\n",
      "4667/4667 [==============================] - 1s 241us/step - loss: 0.4989 - accuracy: 0.7997\n",
      "Epoch 21/100\n",
      "4667/4667 [==============================] - 1s 173us/step - loss: 0.4993 - accuracy: 0.7997\n",
      "Epoch 22/100\n",
      "4667/4667 [==============================] - 1s 182us/step - loss: 0.5001 - accuracy: 0.7997\n",
      "Epoch 23/100\n",
      "4667/4667 [==============================] - 1s 219us/step - loss: 0.4995 - accuracy: 0.7997\n",
      "Epoch 24/100\n",
      "4667/4667 [==============================] - 1s 177us/step - loss: 0.4993 - accuracy: 0.7997\n",
      "Epoch 25/100\n",
      "4667/4667 [==============================] - 1s 187us/step - loss: 0.4982 - accuracy: 0.7997\n",
      "Epoch 26/100\n",
      "4667/4667 [==============================] - 1s 202us/step - loss: 0.5009 - accuracy: 0.7997\n",
      "Epoch 27/100\n",
      "4667/4667 [==============================] - 1s 238us/step - loss: 0.5007 - accuracy: 0.7997\n",
      "Epoch 28/100\n",
      "4667/4667 [==============================] - 1s 198us/step - loss: 0.4999 - accuracy: 0.7997\n",
      "Epoch 29/100\n",
      "4667/4667 [==============================] - 1s 161us/step - loss: 0.5002 - accuracy: 0.7997\n",
      "Epoch 30/100\n",
      "4667/4667 [==============================] - 1s 259us/step - loss: 0.5001 - accuracy: 0.7997\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4667/4667 [==============================] - 1s 196us/step - loss: 0.4998 - accuracy: 0.7997\n",
      "Epoch 32/100\n",
      "4667/4667 [==============================] - 1s 191us/step - loss: 0.5013 - accuracy: 0.7997\n",
      "Epoch 33/100\n",
      "4667/4667 [==============================] - 1s 210us/step - loss: 0.5003 - accuracy: 0.7997\n",
      "Epoch 34/100\n",
      "4667/4667 [==============================] - 1s 190us/step - loss: 0.4997 - accuracy: 0.7997\n",
      "Epoch 35/100\n",
      "4667/4667 [==============================] - 1s 164us/step - loss: 0.4997 - accuracy: 0.7997\n",
      "Epoch 36/100\n",
      "4667/4667 [==============================] - 1s 154us/step - loss: 0.5001 - accuracy: 0.7997\n",
      "Epoch 37/100\n",
      "4667/4667 [==============================] - 1s 161us/step - loss: 0.5001 - accuracy: 0.7997\n",
      "Epoch 38/100\n",
      "4667/4667 [==============================] - 1s 180us/step - loss: 0.4989 - accuracy: 0.7997\n",
      "Epoch 39/100\n",
      "4667/4667 [==============================] - 1s 170us/step - loss: 0.4988 - accuracy: 0.7997\n",
      "Epoch 40/100\n",
      "4667/4667 [==============================] - 1s 197us/step - loss: 0.4993 - accuracy: 0.7997\n",
      "Epoch 41/100\n",
      "4667/4667 [==============================] - 1s 189us/step - loss: 0.4979 - accuracy: 0.7997\n",
      "Epoch 42/100\n",
      "4667/4667 [==============================] - 1s 158us/step - loss: 0.5012 - accuracy: 0.7997\n",
      "Epoch 43/100\n",
      "4667/4667 [==============================] - 1s 169us/step - loss: 0.4998 - accuracy: 0.7997\n",
      "Epoch 44/100\n",
      "4667/4667 [==============================] - 1s 160us/step - loss: 0.4995 - accuracy: 0.7997\n",
      "Epoch 45/100\n",
      "4667/4667 [==============================] - 1s 145us/step - loss: 0.4998 - accuracy: 0.7997\n",
      "Epoch 46/100\n",
      "4667/4667 [==============================] - 1s 160us/step - loss: 0.4993 - accuracy: 0.7997\n",
      "Epoch 47/100\n",
      "4667/4667 [==============================] - 1s 167us/step - loss: 0.4976 - accuracy: 0.7997\n",
      "Epoch 48/100\n",
      "4667/4667 [==============================] - 1s 210us/step - loss: 0.4994 - accuracy: 0.7997\n",
      "Epoch 49/100\n",
      "4667/4667 [==============================] - 1s 196us/step - loss: 0.4996 - accuracy: 0.7997\n",
      "Epoch 50/100\n",
      "4667/4667 [==============================] - 1s 159us/step - loss: 0.4975 - accuracy: 0.7997\n",
      "Epoch 51/100\n",
      "4667/4667 [==============================] - 1s 151us/step - loss: 0.4970 - accuracy: 0.7997\n",
      "Epoch 52/100\n",
      "4667/4667 [==============================] - 1s 193us/step - loss: 0.5002 - accuracy: 0.7997\n",
      "Epoch 53/100\n",
      "4667/4667 [==============================] - 1s 216us/step - loss: 0.5000 - accuracy: 0.7997\n",
      "Epoch 54/100\n",
      "4667/4667 [==============================] - 1s 208us/step - loss: 0.5000 - accuracy: 0.7997\n",
      "Epoch 55/100\n",
      "4667/4667 [==============================] - 1s 161us/step - loss: 0.4995 - accuracy: 0.7997\n",
      "Epoch 56/100\n",
      "4667/4667 [==============================] - 1s 161us/step - loss: 0.4981 - accuracy: 0.7997\n",
      "Epoch 57/100\n",
      "4667/4667 [==============================] - 1s 155us/step - loss: 0.4960 - accuracy: 0.7997\n",
      "Epoch 58/100\n",
      "4667/4667 [==============================] - 1s 152us/step - loss: 0.4969 - accuracy: 0.7997\n",
      "Epoch 59/100\n",
      "4667/4667 [==============================] - 1s 175us/step - loss: 0.4972 - accuracy: 0.7997\n",
      "Epoch 60/100\n",
      "4667/4667 [==============================] - 1s 177us/step - loss: 0.4971 - accuracy: 0.7997\n",
      "Epoch 61/100\n",
      "4667/4667 [==============================] - 1s 189us/step - loss: 0.4961 - accuracy: 0.7997\n",
      "Epoch 62/100\n",
      "4667/4667 [==============================] - 1s 162us/step - loss: 0.4956 - accuracy: 0.7997\n",
      "Epoch 63/100\n",
      "4667/4667 [==============================] - 1s 158us/step - loss: 0.4958 - accuracy: 0.7997\n",
      "Epoch 64/100\n",
      "4667/4667 [==============================] - 1s 132us/step - loss: 0.4974 - accuracy: 0.7997\n",
      "Epoch 65/100\n",
      "4667/4667 [==============================] - 1s 131us/step - loss: 0.4962 - accuracy: 0.7997\n",
      "Epoch 66/100\n",
      "4667/4667 [==============================] - 1s 129us/step - loss: 0.4954 - accuracy: 0.7997\n",
      "Epoch 67/100\n",
      "4667/4667 [==============================] - 1s 133us/step - loss: 0.4948 - accuracy: 0.7997\n",
      "Epoch 68/100\n",
      "4667/4667 [==============================] - 1s 133us/step - loss: 0.4955 - accuracy: 0.7997\n",
      "Epoch 69/100\n",
      "4667/4667 [==============================] - 1s 130us/step - loss: 0.4951 - accuracy: 0.7997\n",
      "Epoch 70/100\n",
      "4667/4667 [==============================] - 1s 130us/step - loss: 0.4956 - accuracy: 0.7997\n",
      "Epoch 71/100\n",
      "4667/4667 [==============================] - 1s 132us/step - loss: 0.4954 - accuracy: 0.7997\n",
      "Epoch 72/100\n",
      "4667/4667 [==============================] - 1s 129us/step - loss: 0.4955 - accuracy: 0.7997\n",
      "Epoch 73/100\n",
      "4667/4667 [==============================] - 1s 134us/step - loss: 0.4961 - accuracy: 0.7997\n",
      "Epoch 74/100\n",
      "4667/4667 [==============================] - 1s 207us/step - loss: 0.4969 - accuracy: 0.7997\n",
      "Epoch 75/100\n",
      "4667/4667 [==============================] - 1s 219us/step - loss: 0.4972 - accuracy: 0.7997\n",
      "Epoch 76/100\n",
      "4667/4667 [==============================] - 1s 153us/step - loss: 0.4972 - accuracy: 0.7997\n",
      "Epoch 77/100\n",
      "4667/4667 [==============================] - 1s 130us/step - loss: 0.4967 - accuracy: 0.7997\n",
      "Epoch 78/100\n",
      "4667/4667 [==============================] - 1s 132us/step - loss: 0.4963 - accuracy: 0.7997\n",
      "Epoch 79/100\n",
      "4667/4667 [==============================] - 1s 129us/step - loss: 0.4960 - accuracy: 0.7997\n",
      "Epoch 80/100\n",
      "4667/4667 [==============================] - 1s 132us/step - loss: 0.4957 - accuracy: 0.7997\n",
      "Epoch 81/100\n",
      "4667/4667 [==============================] - 1s 146us/step - loss: 0.4955 - accuracy: 0.7997\n",
      "Epoch 82/100\n",
      "4667/4667 [==============================] - 1s 131us/step - loss: 0.4954 - accuracy: 0.7997\n",
      "Epoch 83/100\n",
      "4667/4667 [==============================] - 1s 132us/step - loss: 0.4952 - accuracy: 0.7997\n",
      "Epoch 84/100\n",
      "4667/4667 [==============================] - 1s 136us/step - loss: 0.4950 - accuracy: 0.7997\n",
      "Epoch 85/100\n",
      "4667/4667 [==============================] - 1s 141us/step - loss: 0.4960 - accuracy: 0.7997\n",
      "Epoch 86/100\n",
      "4667/4667 [==============================] - 1s 168us/step - loss: 0.4949 - accuracy: 0.7997\n",
      "Epoch 87/100\n",
      "4667/4667 [==============================] - 1s 175us/step - loss: 0.4949 - accuracy: 0.7997\n",
      "Epoch 88/100\n",
      "4667/4667 [==============================] - 1s 143us/step - loss: 0.4946 - accuracy: 0.7997\n",
      "Epoch 89/100\n",
      "4667/4667 [==============================] - 1s 170us/step - loss: 0.4948 - accuracy: 0.7997\n",
      "Epoch 90/100\n",
      "4667/4667 [==============================] - 1s 155us/step - loss: 0.4946 - accuracy: 0.7997\n",
      "Epoch 91/100\n",
      "4667/4667 [==============================] - 1s 127us/step - loss: 0.4947 - accuracy: 0.7997\n",
      "Epoch 92/100\n",
      "4667/4667 [==============================] - 1s 128us/step - loss: 0.4949 - accuracy: 0.7997\n",
      "Epoch 93/100\n",
      "4667/4667 [==============================] - 1s 129us/step - loss: 0.4948 - accuracy: 0.7997\n",
      "Epoch 94/100\n",
      "4667/4667 [==============================] - 1s 134us/step - loss: 0.4948 - accuracy: 0.7997\n",
      "Epoch 95/100\n",
      "4667/4667 [==============================] - 1s 156us/step - loss: 0.4947 - accuracy: 0.7997\n",
      "Epoch 96/100\n",
      "4667/4667 [==============================] - 1s 129us/step - loss: 0.4947 - accuracy: 0.7997\n",
      "Epoch 97/100\n",
      "4667/4667 [==============================] - 1s 135us/step - loss: 0.4946 - accuracy: 0.7997\n",
      "Epoch 98/100\n",
      "4667/4667 [==============================] - 1s 190us/step - loss: 0.4947 - accuracy: 0.7997\n",
      "Epoch 99/100\n",
      "4667/4667 [==============================] - 1s 171us/step - loss: 0.4946 - accuracy: 0.7997\n",
      "Epoch 100/100\n",
      "4667/4667 [==============================] - 1s 130us/step - loss: 0.4949 - accuracy: 0.7997\n",
      "2333/2333 [==============================] - 0s 74us/step\n"
     ]
    }
   ],
   "source": [
    "accuracies = cross_val_score(estimator = kclassifier, X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the mean and the varince of your accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8019993702570597"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = accuracies.mean()\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006385991401441906"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance = accuracies.std()\n",
    "variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Tuning the ANN (something for you to work on at home, if you want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
